{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from openai import OpenAI # Import OpenAI library\n",
    "import json # To potentially parse structured LLM output\n",
    "from dotenv import load_dotenv\n",
    "import openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- IMPORTANT: API Key Configuration ---\n",
    "# Best practice: Store your key in an environment variable or secure config\n",
    "# os.environ['OPENAI_API_KEY'] = 'YOUR_ACTUAL_API_KEY' # <-- NEVER hardcode keys directly in scripts shared or committed\n",
    "# client = OpenAI() # Reads key from environment variable automatically\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI client initialized after loading .env file.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    client = OpenAI()\n",
    "    # Optional: Verify\n",
    "    if os.getenv('OPENAI_API_KEY'):\n",
    "        print(\"OpenAI client initialized after loading .env file.\")\n",
    "    else:\n",
    "        print(\"Warning: OPENAI_API_KEY not found after loading .env. Check .env file location and content.\")\n",
    "    # You can now use the 'client' object\n",
    "    # client.models.list() # Example call\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing OpenAI client after loading .env: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SyncPage[Model](data=[Model(id='gpt-4o-audio-preview-2024-12-17', created=1734034239, object='model', owned_by='system'), Model(id='dall-e-3', created=1698785189, object='model', owned_by='system'), Model(id='dall-e-2', created=1698798177, object='model', owned_by='system'), Model(id='gpt-4o-audio-preview-2024-10-01', created=1727389042, object='model', owned_by='system'), Model(id='gpt-4-turbo-preview', created=1706037777, object='model', owned_by='system'), Model(id='text-embedding-3-small', created=1705948997, object='model', owned_by='system'), Model(id='gpt-4-turbo', created=1712361441, object='model', owned_by='system'), Model(id='gpt-4-turbo-2024-04-09', created=1712601677, object='model', owned_by='system'), Model(id='gpt-4.1-nano', created=1744321707, object='model', owned_by='system'), Model(id='gpt-4.1-nano-2025-04-14', created=1744321025, object='model', owned_by='system'), Model(id='gpt-4o-realtime-preview-2024-10-01', created=1727131766, object='model', owned_by='system'), Model(id='gpt-4o-realtime-preview', created=1727659998, object='model', owned_by='system'), Model(id='babbage-002', created=1692634615, object='model', owned_by='system'), Model(id='gpt-4', created=1687882411, object='model', owned_by='openai'), Model(id='text-embedding-ada-002', created=1671217299, object='model', owned_by='openai-internal'), Model(id='chatgpt-4o-latest', created=1723515131, object='model', owned_by='system'), Model(id='gpt-4o-realtime-preview-2024-12-17', created=1733945430, object='model', owned_by='system'), Model(id='gpt-4o-mini-audio-preview', created=1734387424, object='model', owned_by='system'), Model(id='gpt-4o-audio-preview', created=1727460443, object='model', owned_by='system'), Model(id='o1-preview-2024-09-12', created=1725648865, object='model', owned_by='system'), Model(id='gpt-4o-mini-realtime-preview', created=1734387380, object='model', owned_by='system'), Model(id='gpt-4.1-mini', created=1744318173, object='model', owned_by='system'), Model(id='gpt-4o-mini-realtime-preview-2024-12-17', created=1734112601, object='model', owned_by='system'), Model(id='gpt-3.5-turbo-instruct-0914', created=1694122472, object='model', owned_by='system'), Model(id='gpt-4o-mini-search-preview', created=1741391161, object='model', owned_by='system'), Model(id='gpt-4.1-mini-2025-04-14', created=1744317547, object='model', owned_by='system'), Model(id='o1', created=1734375816, object='model', owned_by='system'), Model(id='o1-2024-12-17', created=1734326976, object='model', owned_by='system'), Model(id='davinci-002', created=1692634301, object='model', owned_by='system'), Model(id='gpt-3.5-turbo-1106', created=1698959748, object='model', owned_by='system'), Model(id='gpt-4o-search-preview', created=1741388720, object='model', owned_by='system'), Model(id='gpt-3.5-turbo-instruct', created=1692901427, object='model', owned_by='system'), Model(id='gpt-3.5-turbo', created=1677610602, object='model', owned_by='openai'), Model(id='gpt-4o-mini-search-preview-2025-03-11', created=1741390858, object='model', owned_by='system'), Model(id='gpt-4-0125-preview', created=1706037612, object='model', owned_by='system'), Model(id='gpt-4o-2024-11-20', created=1739331543, object='model', owned_by='system'), Model(id='whisper-1', created=1677532384, object='model', owned_by='openai-internal'), Model(id='gpt-4o-2024-05-13', created=1715368132, object='model', owned_by='system'), Model(id='o1-pro', created=1742251791, object='model', owned_by='system'), Model(id='gpt-3.5-turbo-16k', created=1683758102, object='model', owned_by='openai-internal'), Model(id='gpt-image-1', created=1745517030, object='model', owned_by='system'), Model(id='o1-pro-2025-03-19', created=1742251504, object='model', owned_by='system'), Model(id='o1-preview', created=1725648897, object='model', owned_by='system'), Model(id='gpt-4-0613', created=1686588896, object='model', owned_by='openai'), Model(id='text-embedding-3-large', created=1705953180, object='model', owned_by='system'), Model(id='gpt-4o-mini-tts', created=1742403959, object='model', owned_by='system'), Model(id='gpt-4o-transcribe', created=1742068463, object='model', owned_by='system'), Model(id='gpt-4.5-preview', created=1740623059, object='model', owned_by='system'), Model(id='gpt-4.5-preview-2025-02-27', created=1740623304, object='model', owned_by='system'), Model(id='gpt-4o-mini-transcribe', created=1742068596, object='model', owned_by='system'), Model(id='gpt-4o-search-preview-2025-03-11', created=1741388170, object='model', owned_by='system'), Model(id='omni-moderation-2024-09-26', created=1732734466, object='model', owned_by='system'), Model(id='o3-mini', created=1737146383, object='model', owned_by='system'), Model(id='o3-mini-2025-01-31', created=1738010200, object='model', owned_by='system'), Model(id='tts-1-hd', created=1699046015, object='model', owned_by='system'), Model(id='gpt-4o', created=1715367049, object='model', owned_by='system'), Model(id='tts-1-hd-1106', created=1699053533, object='model', owned_by='system'), Model(id='gpt-4o-mini', created=1721172741, object='model', owned_by='system'), Model(id='gpt-4o-2024-08-06', created=1722814719, object='model', owned_by='system'), Model(id='gpt-4.1', created=1744316542, object='model', owned_by='system'), Model(id='gpt-4.1-2025-04-14', created=1744315746, object='model', owned_by='system'), Model(id='gpt-4o-mini-2024-07-18', created=1721172717, object='model', owned_by='system'), Model(id='o1-mini', created=1725649008, object='model', owned_by='system'), Model(id='gpt-4o-mini-audio-preview-2024-12-17', created=1734115920, object='model', owned_by='system'), Model(id='gpt-3.5-turbo-0125', created=1706048358, object='model', owned_by='system'), Model(id='o1-mini-2024-09-12', created=1725648979, object='model', owned_by='system'), Model(id='tts-1', created=1681940951, object='model', owned_by='openai-internal'), Model(id='gpt-4-1106-preview', created=1698957206, object='model', owned_by='system'), Model(id='tts-1-1106', created=1699053241, object='model', owned_by='system'), Model(id='omni-moderation-latest', created=1731689265, object='model', owned_by='system'), Model(id='o4-mini-2025-04-16', created=1744133506, object='model', owned_by='system'), Model(id='o4-mini', created=1744225351, object='model', owned_by='system')], object='list')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = OpenAI()\n",
    "client.models.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_file_path = 'data/excel_fooddesert/full_literature_list_2024-10-21(analyzing).xlsx' \n",
    "output_dir_llm = 'llm_outputs' # Directory to save results\n",
    "os.makedirs(output_dir_llm, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Successfully loaded and cleaned Excel file. Shape: (847, 65)\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "if not os.path.exists(excel_file_path):\n",
    "    print(f\"Error: Excel file not found at {excel_file_path}\")\n",
    "    exit()\n",
    "try:\n",
    "    df_lit = pd.read_excel(excel_file_path)\n",
    "    # Remove 'Unnamed' columns\n",
    "    unnamed_cols_mask = df_lit.columns.str.contains('Unnamed:')\n",
    "    df_lit = df_lit.loc[:, ~unnamed_cols_mask]\n",
    "    print(f\"Successfully loaded and cleaned Excel file. Shape: {df_lit.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the Excel file: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cols_for_llm = ['Article Title', 'Abstract', 'Author Keywords', 'Keywords Plus', 'Country', 'Continent']\n",
    "for col in text_cols_for_llm:\n",
    "    if col in df_lit.columns:\n",
    "        df_lit[col] = df_lit[col].fillna('').astype(str)\n",
    "    else:\n",
    "        print(f\"Warning: Column '{col}' not found.\")\n",
    "        if col in ['Country', 'Continent']: # Add placeholder if essential geo cols missing\n",
    "             df_lit[col] = 'Unknown'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define LLM Interaction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_measurement_prompt(title, abstract, keywords):\n",
    "    # Combine keywords for context\n",
    "    all_keywords = f\"{keywords.get('Author Keywords', '')}; {keywords.get('Keywords Plus', '')}\"\n",
    "    prompt = f\"\"\"Analyze the following research paper abstract about food deserts. Focus ONLY on how food deserts or food access were measured or assessed.\n",
    "\n",
    "    Paper Title: {title}\n",
    "    Keywords: {all_keywords}\n",
    "    Abstract: {abstract}\n",
    "\n",
    "    Instructions:\n",
    "    1. Identify the primary methodology used for measurement.\n",
    "    2. Categorize this methodology into ONE of the following types:\n",
    "       - Spatial: Distance/Buffer\n",
    "       - Spatial: Network Analysis\n",
    "       - Spatial: Density Metrics (e.g., store counts, RFEI/mRFEI)\n",
    "       - Temporal Analysis (incorporating time)\n",
    "       - Socioeconomic Integration (explicitly combining spatial with income, race, vehicle access etc.)\n",
    "       - Survey/Qualitative Methods\n",
    "       - Integrated/Mixed-Methods (combining multiple distinct types)\n",
    "       - Not Applicable/Not Specified\n",
    "       - Other (please specify briefly)\n",
    "    3. List the key variables or data types explicitly mentioned as being used in the measurement (e.g., 'supermarket locations', 'census tract income', 'travel time', 'road network', 'household survey data'). List up to 5 key variables.\n",
    "\n",
    "    Provide the output ONLY in JSON format like this:\n",
    "    {{\n",
    "      \"methodology_category\": \"Your Category Here\",\n",
    "      \"key_variables\": [\"Variable 1\", \"Variable 2\", ...]\n",
    "    }}\n",
    "    \"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_intervention_prompt(title, abstract, keywords):\n",
    "    all_keywords = f\"{keywords.get('Author Keywords', '')}; {keywords.get('Keywords Plus', '')}\"\n",
    "    prompt = f\"\"\"Analyze the following research paper abstract about food deserts. Focus ONLY on interventions, policies, solutions, or strategies mentioned to address food deserts or improve food access.\n",
    "\n",
    "    Paper Title: {title}\n",
    "    Keywords: {all_keywords}\n",
    "    Abstract: {abstract}\n",
    "\n",
    "    Instructions:\n",
    "    1. List the specific interventions, policies, or solutions mentioned (e.g., 'SNAP participation', 'supermarket subsidies', 'mobile markets', 'community gardens', 'nutrition education', 'transport improvements').\n",
    "    2. If no interventions are mentioned, state \"None\".\n",
    "\n",
    "    Provide the output ONLY in JSON format like this:\n",
    "    {{\n",
    "      \"interventions_mentioned\": [\"Intervention 1\", \"Intervention 2\", ...] or [\"None\"]\n",
    "    }}\n",
    "    \"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_regional_prompt(title, abstract, keywords, country, continent):\n",
    "     all_keywords = f\"{keywords.get('Author Keywords', '')}; {keywords.get('Keywords Plus', '')}\"\n",
    "     prompt = f\"\"\"Analyze the following research paper abstract about food deserts, specifically considering its regional context ({country}, {continent}).\n",
    "\n",
    "     Paper Title: {title}\n",
    "     Keywords: {all_keywords}\n",
    "     Abstract: {abstract}\n",
    "\n",
    "     Instructions:\n",
    "     1. Briefly summarize (1-2 sentences) the main finding or focus of this study AS IT RELATES to its specific location or context, if mentioned.\n",
    "     2. Does the abstract highlight any unique regional factors (e.g., specific policies, demographic issues like aging, urban form, role of informal markets) influencing food deserts in this location? If yes, list keywords or a brief phrase. If no, state \"None\".\n",
    "\n",
    "     Provide the output ONLY in JSON format like this:\n",
    "     {{\n",
    "       \"context_summary\": \"Brief summary here.\",\n",
    "       \"unique_regional_factors\": [\"Factor 1 phrase\", \"Factor 2 phrase\", ...] or [\"None\"]\n",
    "     }}\n",
    "     \"\"\"\n",
    "     return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_analysis(prompt_text, model=\"gpt-4o-mini\", max_retries=3, delay=1):\n",
    "    \"\"\"Calls the OpenAI API with specified prompt and handles retries.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    # {\"role\": \"system\", \"content\": \"You are a helpful research assistant analyzing abstracts.\"}, # Optional system message\n",
    "                    {\"role\": \"user\", \"content\": prompt_text}\n",
    "                ],\n",
    "                temperature=0.2, # Lower temperature for more focused, less creative output\n",
    "                max_tokens=200, # Adjust as needed\n",
    "                response_format={ \"type\": \"json_object\" } # Request JSON output\n",
    "            )\n",
    "            # Extract the content and parse JSON\n",
    "            content = response.choices[0].message.content\n",
    "            return json.loads(content) # Parse the JSON string response\n",
    "        except Exception as e:\n",
    "            print(f\"API Error: {e}. Attempt {attempt + 1} of {max_retries}.\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(delay * (attempt + 1)) # Exponential backoff\n",
    "            else:\n",
    "                print(\"Max retries reached. Skipping this paper.\")\n",
    "                return None # Return None or an error indicator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Analysis (Sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = df_lit.head(10).copy() # Start with first 10 papers\n",
    "# sample_df = df_lit.iloc[10:20].copy() # Or another slice\n",
    "# Or process the whole dataframe (use with caution re: cost/time)\n",
    "# sample_df = df_lit.copy()\n",
    "\n",
    "results_measurement = []\n",
    "results_intervention = []\n",
    "results_regional = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_measurement_analysis = True\n",
    "run_intervention_analysis = True\n",
    "run_regional_analysis = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 10 papers...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Processing {len(sample_df)} papers...\")\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing paper index: 0...\n",
      "  Measurement Result: {'methodology_category': 'Spatial: Distance/Buffer', 'key_variables': ['shopping behavior', 'travel distance', 'vendor visits', 'supermarket vendor', 'WIC Program participants']}\n",
      "  Intervention Result: {'interventions_mentioned': ['None']}\n",
      "  Regional Result: {'context_summary': 'The study investigates the shopping behaviors of WIC Program participants in food deserts within the Greater Los Angeles area, finding that their shopping patterns are similar to those of participants outside food deserts.', 'unique_regional_factors': ['Greater Los Angeles area', 'WIC Program', 'food deserts']}\n",
      "\n",
      "Processing paper index: 1...\n",
      "  Measurement Result: {'methodology_category': 'Spatial: Density Metrics (e.g., store counts, RFEI/mRFEI)', 'key_variables': ['census blocks', 'pricing strategies', 'location', 'ownership', 'restaurant characteristics']}\n",
      "  Intervention Result: {'interventions_mentioned': ['None']}\n",
      "  Regional Result: {'context_summary': 'The study investigates how fast food restaurant pricing strategies differ in Michigan food deserts compared to non-food deserts, revealing that higher prices are charged for select food items in food deserts despite similar amenities and ownership structures.', 'unique_regional_factors': ['Michigan-specific pricing strategies', 'food desert characteristics']}\n",
      "\n",
      "Processing paper index: 2...\n",
      "  Measurement Result: {'methodology_category': 'Not Applicable/Not Specified', 'key_variables': []}\n",
      "  Intervention Result: {'interventions_mentioned': ['None']}\n",
      "  Regional Result: {'context_summary': 'The study focuses on the challenges faced by vulnerable populations in the food desert of inner city Richmond, Virginia, highlighting how these challenges reflect broader issues of economic, social, cultural, and educational disenfranchisement.', 'unique_regional_factors': ['economic disenfranchisement', 'social disenfranchisement', 'cultural disenfranchisement', 'educational disenfranchisement']}\n",
      "\n",
      "Processing paper index: 3...\n",
      "  Measurement Result: {'methodology_category': 'Socioeconomic Integration', 'key_variables': ['2006 and 2010 Health and Retirement Study data', 'census tract-level measures of food deserts', 'food insufficiency', 'SNAP participation', 'vehicle access']}\n",
      "  Intervention Result: {'interventions_mentioned': ['SNAP participation', 'subsidized meals']}\n",
      "  Regional Result: {'context_summary': 'The study investigates the impact of food deserts on food insufficiency and SNAP participation among low-income elderly residents in urban counties in the USA, finding that those without vehicles in food deserts are particularly vulnerable to food insufficiency.', 'unique_regional_factors': ['limited transportation options', 'fixed incomes', 'urban counties', 'SNAP participation']}\n",
      "\n",
      "Processing paper index: 4...\n",
      "  Measurement Result: {'methodology_category': 'Integrated/Mixed-Methods', 'key_variables': ['grocery store locations', 'diet changes', 'food access', 'in-depth interviews', 'social relationships']}\n",
      "  Intervention Result: {'interventions_mentioned': ['None']}\n",
      "  Regional Result: {'context_summary': 'This study investigates the impact of living in a small-town food desert in the USA, revealing that while opening a grocery store does not change diet, it improves food access and affects social relationships.', 'unique_regional_factors': ['small-town dynamics', 'social relationships', 'access to nonlocal grocery stores']}\n",
      "\n",
      "Processing paper index: 5...\n",
      "  Measurement Result: {'methodology_category': 'Socioeconomic Integration', 'key_variables': ['food deserts', 'self-reported dental care utilization', 'poverty rate', 'National Longitudinal Study of Adolescent to Adult Health data', 'covariate-adjusted multiple logistic regression']}\n",
      "  Intervention Result: {'interventions_mentioned': ['None']}\n",
      "  Regional Result: {'context_summary': 'The study finds that living in a food desert in the United States is linked to higher odds of not utilizing dental care, particularly in high-poverty areas with a poverty rate of 20% or more.', 'unique_regional_factors': ['high-poverty areas', 'low-income urban food deserts']}\n",
      "\n",
      "Processing paper index: 6...\n",
      "  Measurement Result: {'methodology_category': 'Spatial: Density Metrics (e.g., store counts, RFEI/mRFEI)', 'key_variables': ['GIS methods', 'definitions of food deserts', 'neighborhoods', 'socioeconomically vulnerable populations', 'food access']}\n",
      "  Intervention Result: {'interventions_mentioned': ['None']}\n",
      "  Regional Result: {'context_summary': 'The study focuses on Portland, Oregon, examining how different definitions of food deserts identify varying neighborhoods, revealing that none effectively capture the majority of socioeconomically vulnerable populations with low food access.', 'unique_regional_factors': ['GIS methods', 'socioeconomically vulnerable populations', 'food hinterland concept']}\n",
      "\n",
      "Processing paper index: 7...\n",
      "  Measurement Result: {'methodology_category': 'Spatial: Density Metrics (e.g., store counts, RFEI/mRFEI)', 'key_variables': ['compactness index', 'census tract', 'sociodemographic characteristics', 'built environmental characteristics', 'odds of being a food desert']}\n",
      "  Intervention Result: {'interventions_mentioned': ['increasing land use density', 'mix of land use', 'improving walkability of neighbourhoods']}\n",
      "  Regional Result: {'context_summary': 'This study investigates the relationship between urban sprawl and the emergence of food deserts in the USA, finding that increased urban compactness is associated with a reduced likelihood of a census tract being classified as a food desert.', 'unique_regional_factors': ['federal and state policy initiatives', 'sociodemographic characteristics', 'built environmental characteristics']}\n",
      "\n",
      "Processing paper index: 8...\n",
      "  Measurement Result: {'methodology_category': 'Spatial: Density Metrics', 'key_variables': ['density of healthy establishments', 'density of unhealthy establishments', 'income per capita', 'number of literate individuals', 'Health Vulnerability Index (HVI)']}\n",
      "  Intervention Result: {'interventions_mentioned': ['None']}\n",
      "  Regional Result: {'context_summary': 'The study investigates the presence of food deserts and food swamps in Belo Horizonte, Brazil, highlighting that neighborhoods classified as food deserts experience greater social inequalities, including lower income and literacy rates compared to food swamps.', 'unique_regional_factors': ['Belo Horizonte', 'social inequality', 'Health Vulnerability Index', 'Brazilian methodology']}\n",
      "\n",
      "Processing paper index: 9...\n",
      "  Measurement Result: {'methodology_category': 'Spatial: Density Metrics (e.g., store counts, RFEI/mRFEI)', 'key_variables': ['store-level scanner data', 'census-tract level Exact Price Index (EPI)', 'food basket defined by the Thrifty Food Plan (TFP)', 'low-income, low-access census tracts', 'variety availability']}\n",
      "  Intervention Result: {'interventions_mentioned': ['None']}\n",
      "  Regional Result: {'context_summary': 'The study investigates the cost of a nutritious diet in food deserts across the USA, finding that while the overall price impact is small, consumers limited to shopping within food deserts face significantly higher prices due to lower variety availability.', 'unique_regional_factors': ['low-income areas', 'low-access census tracts', 'nationally representative sample']}\n"
     ]
    }
   ],
   "source": [
    "for index, row in sample_df.iterrows():\n",
    "    print(f\"\\nProcessing paper index: {index}...\")\n",
    "    title = row.get('Article Title', '')\n",
    "    abstract = row.get('Abstract', '')\n",
    "    keywords = {'Author Keywords': row.get('Author Keywords', ''), 'Keywords Plus': row.get('Keywords Plus', '')}\n",
    "    country = row.get('Country', 'Unknown')\n",
    "    continent = row.get('Continent', 'Unknown')\n",
    "\n",
    "    # Create prompts\n",
    "    if run_measurement_analysis:\n",
    "        meas_prompt = create_measurement_prompt(title, abstract, keywords)\n",
    "        meas_result = get_llm_analysis(meas_prompt)\n",
    "        results_measurement.append(meas_result)\n",
    "        print(f\"  Measurement Result: {meas_result}\")\n",
    "        time.sleep(1) # Pause between API calls to avoid rate limits\n",
    "\n",
    "    if run_intervention_analysis:\n",
    "        int_prompt = create_intervention_prompt(title, abstract, keywords)\n",
    "        int_result = get_llm_analysis(int_prompt)\n",
    "        results_intervention.append(int_result)\n",
    "        print(f\"  Intervention Result: {int_result}\")\n",
    "        time.sleep(1)\n",
    "\n",
    "    if run_regional_analysis:\n",
    "         reg_prompt = create_regional_prompt(title, abstract, keywords, country, continent)\n",
    "         reg_result = get_llm_analysis(reg_prompt)\n",
    "         results_regional.append(reg_result)\n",
    "         print(f\"  Regional Result: {reg_result}\")\n",
    "         time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_measurement_analysis:\n",
    "    # Extract specific fields from JSON results carefully\n",
    "    sample_df['LLM_Methodology'] = [res.get('methodology_category', 'Error') if res else 'Error' for res in results_measurement]\n",
    "    sample_df['LLM_Variables'] = [res.get('key_variables', []) if res else [] for res in results_measurement]\n",
    "\n",
    "if run_intervention_analysis:\n",
    "    sample_df['LLM_Interventions'] = [res.get('interventions_mentioned', []) if res else [] for res in results_intervention]\n",
    "\n",
    "if run_regional_analysis:\n",
    "     sample_df['LLM_Context_Summary'] = [res.get('context_summary', 'Error') if res else 'Error' for res in results_regional]\n",
    "     sample_df['LLM_Regional_Factors'] = [res.get('unique_regional_factors', []) if res else [] for res in results_regional]\n",
    "\n",
    "\n",
    "print(\"\\nRemember to CRITICALLY REVIEW and VERIFY the LLM's output against the original papers before finalizing your tables.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Regional Context Summaries and Factors:\n",
      "\n",
      "--- Region: North America ---\n",
      "  Number of papers in sample: 9\n",
      "                                       Article Title  \\\n",
      "0  Food Access, Food Deserts, and the Women, Infa...   \n",
      "1  Fast Food Restaurant Pricing Strategies in Mic...   \n",
      "2  Vulnerable populations in food deserts: a case...   \n",
      "\n",
      "                                 LLM_Context_Summary  \\\n",
      "0  The study investigates the shopping behaviors ...   \n",
      "1  The study investigates how fast food restauran...   \n",
      "2  The study focuses on the challenges faced by v...   \n",
      "\n",
      "                                LLM_Regional_Factors  \n",
      "0  [Greater Los Angeles area, WIC Program, food d...  \n",
      "1  [Michigan-specific pricing strategies, food de...  \n",
      "2  [economic disenfranchisement, social disenfran...  \n",
      "\n",
      "--- Region: South America ---\n",
      "  Number of papers in sample: 1\n",
      "                                       Article Title  \\\n",
      "8  Social inequalities in the surrounding areas o...   \n",
      "\n",
      "                                 LLM_Context_Summary  \\\n",
      "8  The study investigates the presence of food de...   \n",
      "\n",
      "                                LLM_Regional_Factors  \n",
      "8  [Belo Horizonte, social inequality, Health Vul...  \n"
     ]
    }
   ],
   "source": [
    "if run_regional_analysis and 'LLM_Context_Summary' in sample_df.columns:\n",
    "     print(\"\\nRegional Context Summaries and Factors:\")\n",
    "     # Group by continent and review LLM summaries and factors\n",
    "     for continent, group in sample_df.groupby('Continent'):\n",
    "         if continent != 'Unknown':\n",
    "             print(f\"\\n--- Region: {continent} ---\")\n",
    "             print(f\"  Number of papers in sample: {len(group)}\")\n",
    "             # Display sample summaries/factors for review\n",
    "             print(group[['Article Title', 'LLM_Context_Summary', 'LLM_Regional_Factors']].head(3))\n",
    "             # Use this grouped info + LLM output to write the regional comparison table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Analysis (All)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total papers to process: 847\n"
     ]
    }
   ],
   "source": [
    "papers_to_process = df_lit.copy()\n",
    "total_papers = len(papers_to_process)\n",
    "print(f\"Total papers to process: {total_papers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_measurement_analysis = True\n",
    "run_intervention_analysis = True\n",
    "run_regional_analysis = True\n",
    "# Saving configuration\n",
    "save_interval = 50  # Save progress every 50 papers\n",
    "api_call_delay = 1  # Seconds to wait between API calls (increase if hitting rate limits)\n",
    "# Output file for progress saving\n",
    "progress_output_filename = os.path.join(output_dir_llm, 'llm_analysis_full_results_PROGRESS.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_measurement = [None] * total_papers\n",
    "results_intervention = [None] * total_papers\n",
    "results_regional = [None] * total_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting LLM processing for 847 papers...\n",
      "Progress will be saved every 50 papers to llm_outputs\\llm_analysis_full_results_PROGRESS.xlsx\n",
      "Estimated time depends heavily on API response times and delays (1s between calls).\n",
      "  Processing paper 10/847... (Elapsed: 56.2s)\n",
      "  Processing paper 20/847... (Elapsed: 126.8s)\n",
      "  Processing paper 30/847... (Elapsed: 193.7s)\n",
      "  Processing paper 40/847... (Elapsed: 259.2s)\n",
      "  Processing paper 50/847... (Elapsed: 323.4s)\n",
      "\n",
      "--- Saving progress at paper 50 ---\n",
      "Progress saved to llm_outputs\\llm_analysis_full_results_PROGRESS.xlsx\n",
      "--- Resuming processing ---\n",
      "  Processing paper 60/847... (Elapsed: 388.9s)\n",
      "  Processing paper 70/847... (Elapsed: 454.9s)\n",
      "  Processing paper 80/847... (Elapsed: 516.5s)\n",
      "  Processing paper 90/847... (Elapsed: 583.6s)\n",
      "  Processing paper 100/847... (Elapsed: 645.5s)\n",
      "\n",
      "--- Saving progress at paper 100 ---\n",
      "Progress saved to llm_outputs\\llm_analysis_full_results_PROGRESS.xlsx\n",
      "--- Resuming processing ---\n",
      "  Processing paper 110/847... (Elapsed: 711.4s)\n",
      "  Processing paper 120/847... (Elapsed: 774.6s)\n",
      "  Processing paper 130/847... (Elapsed: 839.0s)\n",
      "  Processing paper 140/847... (Elapsed: 909.2s)\n",
      "  Processing paper 150/847... (Elapsed: 970.1s)\n",
      "\n",
      "--- Saving progress at paper 150 ---\n",
      "Progress saved to llm_outputs\\llm_analysis_full_results_PROGRESS.xlsx\n",
      "--- Resuming processing ---\n",
      "  Processing paper 160/847... (Elapsed: 1033.3s)\n",
      "  Processing paper 170/847... (Elapsed: 1098.7s)\n",
      "  Processing paper 180/847... (Elapsed: 1163.9s)\n",
      "  Processing paper 190/847... (Elapsed: 1230.1s)\n",
      "  Processing paper 200/847... (Elapsed: 1289.8s)\n",
      "\n",
      "--- Saving progress at paper 200 ---\n",
      "Progress saved to llm_outputs\\llm_analysis_full_results_PROGRESS.xlsx\n",
      "--- Resuming processing ---\n",
      "  Processing paper 210/847... (Elapsed: 1353.7s)\n",
      "  Processing paper 220/847... (Elapsed: 1426.8s)\n",
      "  Processing paper 230/847... (Elapsed: 1492.8s)\n",
      "  Processing paper 240/847... (Elapsed: 1560.7s)\n",
      "  Processing paper 250/847... (Elapsed: 1619.3s)\n",
      "\n",
      "--- Saving progress at paper 250 ---\n",
      "Progress saved to llm_outputs\\llm_analysis_full_results_PROGRESS.xlsx\n",
      "--- Resuming processing ---\n",
      "  Processing paper 260/847... (Elapsed: 1681.1s)\n",
      "  Processing paper 270/847... (Elapsed: 1743.5s)\n",
      "  Processing paper 280/847... (Elapsed: 1808.3s)\n",
      "  Processing paper 290/847... (Elapsed: 1870.1s)\n",
      "  Processing paper 300/847... (Elapsed: 1932.8s)\n",
      "\n",
      "--- Saving progress at paper 300 ---\n",
      "Progress saved to llm_outputs\\llm_analysis_full_results_PROGRESS.xlsx\n",
      "--- Resuming processing ---\n",
      "  Processing paper 310/847... (Elapsed: 1998.4s)\n",
      "  Processing paper 320/847... (Elapsed: 2063.0s)\n",
      "  Processing paper 330/847... (Elapsed: 2122.1s)\n",
      "  Processing paper 340/847... (Elapsed: 2191.4s)\n",
      "  Processing paper 350/847... (Elapsed: 2266.2s)\n",
      "\n",
      "--- Saving progress at paper 350 ---\n",
      "Progress saved to llm_outputs\\llm_analysis_full_results_PROGRESS.xlsx\n",
      "--- Resuming processing ---\n",
      "  Processing paper 360/847... (Elapsed: 2335.4s)\n",
      "  Processing paper 370/847... (Elapsed: 2401.0s)\n",
      "  Processing paper 380/847... (Elapsed: 2463.4s)\n",
      "  Processing paper 390/847... (Elapsed: 2529.2s)\n",
      "  Processing paper 400/847... (Elapsed: 2591.0s)\n",
      "\n",
      "--- Saving progress at paper 400 ---\n",
      "Progress saved to llm_outputs\\llm_analysis_full_results_PROGRESS.xlsx\n",
      "--- Resuming processing ---\n",
      "  Processing paper 410/847... (Elapsed: 2659.0s)\n",
      "  Processing paper 420/847... (Elapsed: 2724.5s)\n",
      "  Processing paper 430/847... (Elapsed: 2784.0s)\n",
      "  Processing paper 440/847... (Elapsed: 2852.2s)\n",
      "  Processing paper 450/847... (Elapsed: 2914.3s)\n",
      "\n",
      "--- Saving progress at paper 450 ---\n",
      "Progress saved to llm_outputs\\llm_analysis_full_results_PROGRESS.xlsx\n",
      "--- Resuming processing ---\n",
      "  Processing paper 460/847... (Elapsed: 2976.3s)\n",
      "  Processing paper 470/847... (Elapsed: 3039.1s)\n",
      "  Processing paper 480/847... (Elapsed: 3098.5s)\n",
      "  Processing paper 490/847... (Elapsed: 3162.3s)\n",
      "  Processing paper 500/847... (Elapsed: 3224.8s)\n",
      "\n",
      "--- Saving progress at paper 500 ---\n",
      "Progress saved to llm_outputs\\llm_analysis_full_results_PROGRESS.xlsx\n",
      "--- Resuming processing ---\n",
      "  Processing paper 510/847... (Elapsed: 3286.2s)\n",
      "  Processing paper 520/847... (Elapsed: 3350.8s)\n",
      "  Processing paper 530/847... (Elapsed: 3417.7s)\n",
      "  Processing paper 540/847... (Elapsed: 3480.0s)\n",
      "  Processing paper 550/847... (Elapsed: 3543.2s)\n",
      "\n",
      "--- Saving progress at paper 550 ---\n",
      "Progress saved to llm_outputs\\llm_analysis_full_results_PROGRESS.xlsx\n",
      "--- Resuming processing ---\n",
      "  Processing paper 560/847... (Elapsed: 3608.0s)\n",
      "  Processing paper 570/847... (Elapsed: 3671.6s)\n",
      "  Processing paper 580/847... (Elapsed: 3735.0s)\n",
      "  Processing paper 590/847... (Elapsed: 3803.1s)\n",
      "  Processing paper 600/847... (Elapsed: 3860.4s)\n",
      "\n",
      "--- Saving progress at paper 600 ---\n",
      "Progress saved to llm_outputs\\llm_analysis_full_results_PROGRESS.xlsx\n",
      "--- Resuming processing ---\n",
      "  Processing paper 610/847... (Elapsed: 3924.4s)\n",
      "  Processing paper 620/847... (Elapsed: 3988.6s)\n",
      "  Processing paper 630/847... (Elapsed: 4053.3s)\n",
      "  Processing paper 640/847... (Elapsed: 4119.4s)\n",
      "  Processing paper 650/847... (Elapsed: 4183.4s)\n",
      "\n",
      "--- Saving progress at paper 650 ---\n",
      "Progress saved to llm_outputs\\llm_analysis_full_results_PROGRESS.xlsx\n",
      "--- Resuming processing ---\n",
      "  Processing paper 660/847... (Elapsed: 4250.1s)\n",
      "  Processing paper 670/847... (Elapsed: 4320.4s)\n",
      "  Processing paper 680/847... (Elapsed: 4384.8s)\n",
      "  Processing paper 690/847... (Elapsed: 4447.6s)\n",
      "  Processing paper 700/847... (Elapsed: 4512.2s)\n",
      "\n",
      "--- Saving progress at paper 700 ---\n",
      "Progress saved to llm_outputs\\llm_analysis_full_results_PROGRESS.xlsx\n",
      "--- Resuming processing ---\n",
      "  Processing paper 710/847... (Elapsed: 4579.8s)\n",
      "  Processing paper 720/847... (Elapsed: 4647.1s)\n",
      "  Processing paper 730/847... (Elapsed: 4713.4s)\n",
      "  Processing paper 740/847... (Elapsed: 4777.8s)\n",
      "  Processing paper 750/847... (Elapsed: 4842.4s)\n",
      "\n",
      "--- Saving progress at paper 750 ---\n",
      "Progress saved to llm_outputs\\llm_analysis_full_results_PROGRESS.xlsx\n",
      "--- Resuming processing ---\n",
      "  Processing paper 760/847... (Elapsed: 4904.0s)\n",
      "  Processing paper 770/847... (Elapsed: 4969.2s)\n",
      "  Processing paper 780/847... (Elapsed: 5037.0s)\n",
      "  Processing paper 790/847... (Elapsed: 5101.4s)\n",
      "  Processing paper 800/847... (Elapsed: 5163.0s)\n",
      "\n",
      "--- Saving progress at paper 800 ---\n",
      "Progress saved to llm_outputs\\llm_analysis_full_results_PROGRESS.xlsx\n",
      "--- Resuming processing ---\n",
      "  Processing paper 810/847... (Elapsed: 5224.3s)\n",
      "  Processing paper 820/847... (Elapsed: 5286.6s)\n",
      "  Processing paper 830/847... (Elapsed: 5357.6s)\n",
      "  Processing paper 840/847... (Elapsed: 5420.5s)\n",
      "\n",
      "--- Saving progress at paper 847 ---\n",
      "Progress saved to llm_outputs\\llm_analysis_full_results_PROGRESS.xlsx\n",
      "--- Resuming processing ---\n",
      "\n",
      ">>> Finished processing 847 papers in 91.29 minutes. <<<\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nStarting LLM processing for {total_papers} papers...\")\n",
    "print(f\"Progress will be saved every {save_interval} papers to {progress_output_filename}\")\n",
    "print(f\"Estimated time depends heavily on API response times and delays ({api_call_delay}s between calls).\")\n",
    "start_time_full = time.time()\n",
    "\n",
    "# --- Main Processing Loop ---\n",
    "for index, row in papers_to_process.iterrows():\n",
    "    # Get the integer position for storing results correctly in lists\n",
    "    list_index = papers_to_process.index.get_loc(index)\n",
    "\n",
    "    # --- Reporting Progress ---\n",
    "    if (list_index + 1) % 10 == 0: # Print status update every 10 papers\n",
    "        elapsed_time = time.time() - start_time_full\n",
    "        print(f\"  Processing paper {list_index + 1}/{total_papers}... (Elapsed: {elapsed_time:.1f}s)\")\n",
    "\n",
    "    # --- Extract data for prompts ---\n",
    "    title = row.get('Article Title', '')\n",
    "    abstract = row.get('Abstract', '')\n",
    "    # Ensure keywords are passed as a dictionary or adjust prompt functions\n",
    "    keywords = {'Author Keywords': row.get('Author Keywords', ''), 'Keywords Plus': row.get('Keywords Plus', '')}\n",
    "    country = row.get('Country', 'Unknown')\n",
    "    continent = row.get('Continent', 'Unknown')\n",
    "\n",
    "    # --- Call LLM for each analysis type ---\n",
    "    # Wrap calls in try-except blocks within the loop if needed for more granular error handling\n",
    "\n",
    "    if run_measurement_analysis:\n",
    "        if list_index < len(results_measurement): # Check index bounds\n",
    "             meas_prompt = create_measurement_prompt(title, abstract, keywords)\n",
    "             meas_result = get_llm_analysis(meas_prompt, delay=api_call_delay)\n",
    "             results_measurement[list_index] = meas_result # Store result at correct position\n",
    "             time.sleep(api_call_delay) # Pause between API calls\n",
    "        else: print(f\"Warning: Index {list_index} out of bounds for results_measurement\")\n",
    "\n",
    "\n",
    "    if run_intervention_analysis:\n",
    "         if list_index < len(results_intervention):\n",
    "             int_prompt = create_intervention_prompt(title, abstract, keywords)\n",
    "             int_result = get_llm_analysis(int_prompt, delay=api_call_delay)\n",
    "             results_intervention[list_index] = int_result # Store result\n",
    "             time.sleep(api_call_delay)\n",
    "         else: print(f\"Warning: Index {list_index} out of bounds for results_intervention\")\n",
    "\n",
    "\n",
    "    if run_regional_analysis:\n",
    "         if list_index < len(results_regional):\n",
    "             reg_prompt = create_regional_prompt(title, abstract, keywords, country, continent)\n",
    "             reg_result = get_llm_analysis(reg_prompt, delay=api_call_delay)\n",
    "             results_regional[list_index] = reg_result # Store result\n",
    "             time.sleep(api_call_delay)\n",
    "         else: print(f\"Warning: Index {list_index} out of bounds for results_regional\")\n",
    "\n",
    "\n",
    "    # --- Periodic Saving ---\n",
    "    if (list_index + 1) % save_interval == 0 or (list_index + 1) == total_papers: # Save every N or at the very end\n",
    "        print(f\"\\n--- Saving progress at paper {list_index + 1} ---\")\n",
    "        # Add results collected SO FAR to a temporary DataFrame for saving\n",
    "        temp_save_df = papers_to_process.copy() # Start with original data\n",
    "\n",
    "        # --- Add LLM result columns to the temp dataframe ---\n",
    "        # Use helper function to safely extract from potentially None results\n",
    "        def safe_get(result, key, default='Error'):\n",
    "            return result.get(key, default) if isinstance(result, dict) else default\n",
    "\n",
    "        def safe_get_list(result, key, default=[]):\n",
    "            # Return list as string for Excel compatibility\n",
    "            return str(result.get(key, default)) if isinstance(result, dict) else str(default)\n",
    "\n",
    "        if run_measurement_analysis:\n",
    "             temp_save_df['LLM_Methodology'] = [safe_get(res, 'methodology_category') for res in results_measurement]\n",
    "             temp_save_df['LLM_Variables'] = [safe_get_list(res, 'key_variables') for res in results_measurement]\n",
    "        if run_intervention_analysis:\n",
    "             temp_save_df['LLM_Interventions'] = [safe_get_list(res, 'interventions_mentioned') for res in results_intervention]\n",
    "        if run_regional_analysis:\n",
    "             temp_save_df['LLM_Context_Summary'] = [safe_get(res, 'context_summary') for res in results_regional]\n",
    "             temp_save_df['LLM_Regional_Factors'] = [safe_get_list(res, 'unique_regional_factors') for res in results_regional]\n",
    "        # --- End of adding LLM result columns ---\n",
    "\n",
    "        try:\n",
    "            # Select only relevant columns + LLM outputs to save space if needed\n",
    "            cols_to_save = ['UT (Unique WOS ID)', 'Authors', 'Publication Year', 'Article Title', 'Abstract', 'Author Keywords', 'Keywords Plus', 'Country', 'Continent', 'Research site'] + [col for col in temp_save_df.columns if col.startswith('LLM_')]\n",
    "            # Ensure all columns exist before trying to save\n",
    "            cols_to_save = [col for col in cols_to_save if col in temp_save_df.columns]\n",
    "            temp_save_df[cols_to_save].to_excel(progress_output_filename, index=False, engine='openpyxl')\n",
    "            print(f\"Progress saved to {progress_output_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving progress: {e}\")\n",
    "        print(\"--- Resuming processing ---\")\n",
    "\n",
    "\n",
    "# --- Final Processing after Loop ---\n",
    "end_time_full = time.time()\n",
    "total_duration_minutes = (end_time_full - start_time_full) / 60\n",
    "print(f\"\\n>>> Finished processing {total_papers} papers in {total_duration_minutes:.2f} minutes. <<<\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display the results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yj_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
